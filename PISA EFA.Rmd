---
title: "An Exploration of Factors that Explain Maths Achivement"
author: "Victor Saidi Phiri"
date: "\today{}"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document:
    fig_caption: yes
    number_sections: yes
header-includes:
- \usepackage{float}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{multicol}
- \usepackage{longtable}
- \usepackage{dcolumn}
geometry: margin=2cm
---
\newpage 
\tableofcontents 
\newpage

```{r setup,include=FALSE}
require("knitr")
require("summarytools")
knitr::opts_chunk$set(echo = F, warning=FALSE, message = FALSE, comment=NA,prompt = FALSE, cache = FALSE, results = 'asis',fig.width = 8,fig.height = 6,results = 'asis',tidy.opts = list(width.cutoff = 60), tidy = TRUE)
 

st_options(bootstrap.css     = FALSE,       # Already part of the theme so no need for it
           plain.ascii       = FALSE,       # One of the essential settings
           style             = "rmarkdown", # Idem.
           dfSummary.silent  = TRUE,        # Suppresses messages about temporary files
           footnote          = NA,          # Keeping the results minimalistic
           subtitle.emphasis = T)       # For the vignette theme, this gives better results.
                                            # For other themes, using TRUE might be preferable.

st_css()
```


```{r}
library(haven)
library(skimr)
library(Amelia)
library(tidyverse)
library(tidyr)
library(psychotools)
library(psych)
library(dplyr)
library(ggplot2)
library(faoutlier)
library(sem)
library(mvtnorm)
library(parallel)
library(GGally) # for ggcorr	
library(corrr) # network_plot	
library(ggcorrplot) # for ggcorrplot	
library(FactoMineR) # multiple PCA functions	
library(factoextra) # visualisation functions for PCA (e.g. fviz_pca_var)	
library(moments)
library(EFAtools)
library(GPArotation)
library(summarytools)
 
library(QuantPsyc)
library(plyr)
library(dplyr)
library(lme4)
library(ggmosaic)
library(nlme)
library(merTools)
library(lmerTest)
library(forcats)
#library(kableExtra)
 
```


```{r,Load custom function}

fviz_loadnings_with_cor <- function(mod, axes = 1, loadings_above = 0.4){	
  require(factoextra)	
  require(dplyr)	
  require(ggplot2)	
if(!is.na(as.character(mod$call$call)[1])){	
  if(as.character(mod$call$call)[1] == "PCA"){	
  contrib_and_cov = as.data.frame(rbind(mod[["var"]][["contrib"]], mod[["var"]][["cor"]]))	
	
vars = rownames(mod[["var"]][["contrib"]])	
attribute_type = rep(c("contribution","correlation"), each = length(vars))	
contrib_and_cov = cbind(contrib_and_cov, attribute_type)	
contrib_and_cov	
	
plot_data = cbind(as.data.frame(cbind(contrib_and_cov[contrib_and_cov[,"attribute_type"] == "contribution",axes], contrib_and_cov[contrib_and_cov[,"attribute_type"] == "correlation",axes])), vars)	
names(plot_data) = c("contribution", "correlation", "vars")	
	
plot_data = plot_data %>% 	
  mutate(correlation = round(correlation, 2))	
	
plot = plot_data %>% 	
  ggplot() +	
  aes(x = reorder(vars, contribution), y = contribution, gradient = correlation, label = correlation)+	
  geom_col(aes(fill = correlation)) +	
  geom_hline(yintercept = mean(plot_data$contribution), col = "red", lty = "dashed") + scale_fill_gradient2() +	
  xlab("variable") +	
  coord_flip() +	
  geom_label(color = "black", fontface = "bold", position = position_dodge(0.5))	
	
	
}	
} else if(!is.na(as.character(mod$Call)[1])){	
  	
  if(as.character(mod$Call)[1] == "fa"){	
    loadings_table = mod$loadings %>% 	
      matrix(ncol = ncol(mod$loadings)) %>% 	
      as_tibble() %>% 	
      mutate(variable = mod$loadings %>% rownames()) %>% 	
      gather(factor, loading, -variable) %>% 	
      mutate(sign = if_else(loading >= 0, "positive", "negative"))	
  	
  if(!is.null(loadings_above)){	
    loadings_table[abs(loadings_table[,"loading"]) < loadings_above,"loading"] = NA	
    loadings_table = loadings_table[!is.na(loadings_table[,"loading"]),]	
  }	
  	
  if(!is.null(axes)){	
  	
  loadings_table = loadings_table %>% 	
     filter(factor == paste0("V",axes))	
  }	
  	
  	
  plot = loadings_table %>% 	
      ggplot() +	
      aes(y = loading %>% abs(), x = reorder(variable, abs(loading)), fill = loading, label =       round(loading, 2)) +	
      geom_col(position = "dodge") +	
      scale_fill_gradient2() +	
      coord_flip() +	
      geom_label(color = "black", fill = "white", fontface = "bold", position = position_dodge(0.5)) +	
      facet_wrap(~factor) +	
      labs(y = "Loading strength", x = "Variable")	
  }	
}	
return(plot)	
	
}	

```


```{r Load datset}
library(haven)
library(tidyverse)
df <- read_sav("PISA15.sav")
```

 

```{r Data cleaning}
#We removed NAs because they were five percent of the data

df<-df %>% drop_na()

```
## Introduction

The essence of this paper is to explore the factors that determine students’ maths scores. A general research topic formulated from the dataset called PISA15. This paper is in two parts; firstly, we shall conduct the exploratory factor analysis to identify factors that explain learners’ performance. Secondly, I shall conduct a regression analysis based on the factor loadings of the EFA. 

The paper starts with a brief background of the study; research questions and hypotheses; information about variables, statistical methods, and justification; results presentation as well as interpretation, and thereafter a discussion of the findings.  

## Background 

Previous scholars have identifed factors that affects learners academic achievement. A study on the influence of students' attitudes on reading achievement by Martinez, Aricak, and Jewel (2008) demonstrated that there is a relationship between reading attitudes and reading achievement through the path model testing relations. Besides that, so many studies have been conducted in the past that have demonstrated the relationship between reading attitudes and reading achievement. Dawkins (2013) also adds that most studies on reading achievement have used Bronfenbrenner's theory on the study and understand how a home or school environments influence students reading achievement.  

*Research questions*:

What are the predictors of maths achievement in Sweden?

what is the best model that explains mathematics scores among learners in Sweden?


## Methods 
### Exploratory Factor Analysis 

We started by assessing the normality assumptions of our data.Skewness > 2.0 or kurtosis > 7.0 would indicate severe univariate nonnormality (Curran et al., 1996). These univariate statistics seem to indicate that all eight measured variables are relatively normally distributed (skew < 1.0 and kurtosis < 2.0)
  

```{r create a correlation matrix}
#we start by selecting the variables of interest. All other variables will remain in the original dataset.

df_selected <-subset(df,select = SUPPORT1:LIKESCI5)

df_cor=cor(df_selected)

```


```{r Visualise the correlation structure}
ggcorrplot::ggcorrplot(df_cor)
```
 

```{r Network Analysis}
library(corrr)
network_plot(df_cor,min_cor=0.6)
 
```

### Assumptions for Factorability

We further used the Kaiser-Meyer-Olkin test to examine factorability within our dataset."A KMO value is a ratio of the sum of squared correlations to the sum of squared correlations plus the sum of squared partial correlations". A minimum score of 0.6 is acceptable for EFA- (*Mvududu & Sink, 2013; Watson, 2017)*. The overall results of the KOM test were 0.90, which is more significant than the 0.6 minimal thresholds. Hence, we concluded that our correlation matrix was appropriate for EFA. 
In addition both the Multivariate Normality test based on skewness and mvn found a p value of less than 0.05 indicating the data is appropriate for EFA. 

```{r   Kaiser-Meyer-Olkin (KMO) test}
 KMO(df_cor)
```


```{r cortest.bartlett}
 cortest.bartlett(df_cor,3977)
 
```

```{r MultivariateNormality mvnTest}

library(MVN) # for mvn function

Output <- mvn(df_selected, mvnTest = "hz")

Output$multivariateNormality
```

The score of less than 0.05 violates the assumption of multivariate skewness

```{r multivariate skew}
library(ICS) # for multivariate skew and kurtosis test
mvnorm.skew.test(df_selected)

```
## Choosing ideal number of factors

we run the parallel test, VSS technique and Kaiser Gutman criterion to determine the number of factors to include in our model.Using these three criteria, it appeared that 8 or 10 factors would be sufficient for an optimal balance between comprehensiveness and parsimony. Since both Velicer MAP, VSS and Kaiser Gutman criterion suggest 8 factors, we shall created our model with 8 factors.

```{r Parallel Test}
fa.parallel(df_cor, n.obs = nrow(df_selected),nfactors = 10, fa = "fa",
            fm = "pa") 
```



```{r,VSS technique}
nfactors(df_cor, n.obs = nrow(df_selected)) 
```


```{r,Kaiser Gutman criterion}
library(FactoMineR)
KGC(df_cor,eigen_type = "EFA",n_factors = 10)
```

#### create the first model

variable **LIKESCI2** (I like reading about science topics) has a highest communality value of 90% with the lowest being 20% **TEABUL1**(teachers called me less often than they called other students)

```{r}
mod1 <- fa(df_cor, nfactors = 8, fm = "pa")
mod1_comm<-as.data.frame(sort(mod1$communality, decreasing = TRUE))
mod1_comm %>% knitr::kable(digits = 2)

```

### Factor Rotation

we prefer oblique rotation due to their accuracy and to honor the reality that most variables are correlated to some extent (Bandalos & Boehm-Kaufman, 2009). Schmitt (2011), argues that "because oblique rotation methods generally produce accurate and comparable factor structures to orthogonal methods even when interfactor correlations are negligible, it is strongly recommend that researchers only use oblique rotation methods because they generally result in more realistic and more statistically sound factor structures" (p. 312).

```{r Structure without rotation}
fa.diagram(mod1)
```

#### PROMAX ROTATION

Among the potential oblique analytic rotations, promax was chosen because it is an oblique modification of the widely accepted varimax procedure (Gorsuch, 1983; Thompson, 2004). I preferred using oblique rotation due to its accuracy and to honor the reality that most variables are correlated to some extent (Bandalos & Boehm-Kaufman, 2009). Schmitt (2011) argues that “because oblique rotation methods generally produce accurate and comparable factor structures to orthogonal methods even when interfactor correlations are negligible, it is strongly recommended that researchers only use oblique rotation methods because they generally result in more realistic and more statistically sound factor structures” (p. 312). Further, oblique rotation gives the same findings as Varimax in the case of uncorrelated data.


```{r}
EFA_promax <- fa(df_cor, nfactors = 8,
                 fm="pa", rotate = "promax")# promax rotation 
comm_EFA_promx<-as.data.frame(sort(EFA_promax$communality, decreasing = TRUE))
#fa.diagram(EFA_promax)
comm_EFA_promx%>% knitr::kable(digits = 2)
```

#### Varimax Rotation

The varimax rotation does not consider the correlation between PA1,PA6,PA7. hence will we use the promax rotation.

```{r}
EFA_varimax <- fa(df_cor, nfactors = 8, 
                  fm="pa", rotate = "varimax")# varimax rotation 

#fa.diagram(EFA_varimax)

```

#### Factor Loadings

Comrey and Lee (1992) suggested that loadings greater than **.70** are excellent, **.63** are very good, **.55** are good, **.45** are fair, and **.32** are poor. Morin et al. (2020) concluded that loadings ≥.**50** are "fully satisfactory" (p. 1052). Hence we will accept loadings above **.55**.
I removed all the variables with loadings below .55 and community below .50(TEABUL1, AMBI4, AMBI3, TEABUL2, WORRY5, TEABUL3). After this adjustment, I conducted the KGC, VSS, and parallel scree plots. Both with the majority tests agreed on 8 factors. 

```{r}

fviz_loadnings_with_cor(EFA_promax, axes = 1, loadings_above = 0.40)
fviz_loadnings_with_cor(EFA_promax, axes = 8, loadings_above = 0.40)


```
 

```{r Select variables}
#We redo the process create the subset - by removing valuables with less influence
df_w2 <- subset(df_selected, 
                select = -c(TEABUL1,TEABUL2,TEABUL3,TEABUL4,AMBI4,AMBI3,WORRY5))

df_Cor2 <- cor(df_w2)

```



```{r parallel Test}
fa.parallel(df_Cor2, n.obs = nrow(df_w2), 
            fa = "fa", fm = "pa")

```


```{r VSS}
nfactors(df_Cor2, n.obs = nrow(df_w2))
```


```{r KGc}
 KGC(df_Cor2,eigen_type = "EFA",n_factors = 8)
```


```{r 8 factor model}
EFA_mod3 <- fa(df_Cor2, nfactors = 8, 
               fm = "pa", rotate = "promax")
EFA_mod3

```
 
```{r Final model}
fa.diagram(EFA_mod3,cut=0.5)# fator analysis structure 
```

```{r }
fviz_loadnings_with_cor(EFA_mod3, axes = 1, loadings_above = 0.55)# loadings 

fviz_loadnings_with_cor(EFA_mod3, axes = 2, loadings_above = 0.55)#loadings
fviz_loadnings_with_cor(EFA_mod3, axes = 3, loadings_above = 0.55)
fviz_loadnings_with_cor(EFA_mod3, axes = 4, loadings_above = 0.55)
fviz_loadnings_with_cor(EFA_mod3, axes = 5, loadings_above = 0.55)
fviz_loadnings_with_cor(EFA_mod3, axes = 6, loadings_above = 0.55)
fviz_loadnings_with_cor(EFA_mod3, axes = 7, loadings_above = 0.55)
fviz_loadnings_with_cor(EFA_mod3, axes = 8, loadings_above = 0.55)
```

##### Post extraction communality

LIKESCI2(I like reading about science topic) and LIKESCI2(I'm Interested in Learning about Science) had the highest post extraction community percentage of 90%.while the lowest score was 50%AMB15(I want to be the best in class).


```{r}
EFA_mod3_common <- as.data.frame(sort(EFA_mod3$communality, 
                                      decreasing = TRUE))

EFA_mod3_common %>% knitr::kable(caption = "post Extration Communality Table",
                                 digits = 2)  
mean(EFA_mod3$communality)

```

```{r  Factor scores}
#we join the factor scores to the main dataset
factorscores = factor.scores(df_w2, EFA_mod3)$scores
variable_factorscores = cbind(df, factorscores)
```


```{r Rename factors}

variable_factorscores$TEASUPPORT=variable_factorscores$PA1
variable_factorscores$LIKES=variable_factorscores$PA2
variable_factorscores$BELONG=variable_factorscores$PA3
variable_factorscores$PARENTSP=variable_factorscores$PA4
variable_factorscores$WORRY=variable_factorscores$PA5
variable_factorscores$DISCORD=variable_factorscores$PA6
variable_factorscores$TEABUL=variable_factorscores$PA7
variable_factorscores$AMBITIONS=variable_factorscores$PA8

```


```{r save dataset with a new name}
df_clean<-variable_factorscores
df_final<-variable_factorscores
```

```{r Recod gender into factor}

df_final$GENDER<- factor(df_final$GENDER)
 
df_final$GENDER<- recode_factor(df_final$GENDER,"1"="Female","2"="Male")
df_reg<- df_final %>% dplyr::select(BOOKS,GENDER,MATHSCORE,SCHOOLID,
                             BELONG,LIKES,TEASUPPORT,
                             DISCORD,WORRY,PARENTSP,TEABUL,AMBITIONS)
```


## REGRESSION ANALYSIS



```{r  create training and test dataset}
#We split the data into training and testing set. 70% training while 30% testing. We use catTools package.
set.seed(1)
library(caTools)
sample <- sample.split(df_reg$GENDER, SplitRatio = 0.70)
train  <- subset(df_reg, sample == TRUE)
test   <- subset(df_reg, sample == FALSE)

train$BOOKS<-as.numeric(train$BOOKS)
```

 
```{r}
table1::table1(~BOOKS+MATHSCORE+
                             BELONG+LIKES+TEASUPPORT+
                             DISCORD+WORRY+PARENTSP+TEABUL+AMBITIONS|GENDER,caption="Descriptive Statistics",
                  data = train,transpose=FALSE,
               topclass="Rtable1-grid Rtable1-shade Rtable1-times") %>% knitr::kable()
```
### Visualise

```{r}
ggplot(train,aes(GENDER,MATHSCORE,fill=GENDER))+
  geom_boxplot(outlier.shape = 8,outlier.colour = "red")+
  theme_bw()
```
#Dependant variable 
The data is normally distributed 
```{r}
ggplot(train,aes(MATHSCORE,..density..))+
  geom_histogram()+
  geom_density()+
  geom_vline(aes(xintercept = mean(MATHSCORE)),
                 color="red",linetype="dashed")+
  theme_bw()
```
```{r}
mlm.check <- function(df,dv){
  df = subset(df,!is.na(get(dv)))
  intercept.only <- gls(as.formula(paste(dv,"~1",sep = "")),data = df)
  random.intercept.only <- lme(as.formula(paste(dv,"~ 1",sep = "")), random = ~1|SCHOOLID,data = df)
  #Do we need a multilevel model? 
  y = ICC(outcome = dv,group = "SCHOOLID",data = df)
  W = anova(intercept.only,random.intercept.only)
  W[1,1] = "Intercept Only"
  W[2,1] = "Random Intercept"
  row.names(W) = NULL
  print(knitr::kable(W, caption = "ANOVA Between Intercept Only and Random Intercept Model"))
  return(y)
}





my.model <- function(df,dv,pred){
  MM <- lmer(paste(dv,"~",pred,"+ (1|SCHOOLID)",sep = ""),data =df)
  
  results = as.data.frame(summary(MM)$coefficients)
  results = results %>% dplyr::mutate(Star = cut(results[,5],breaks = c(0,0.001,0.01,0.05,1.1),right = FALSE))
  results$Star = mapvalues(results$Star,from = levels(results$Star), to = c("***","**","*",""))
  results = results[,-c(3,4)]
  print(knitr::kable(results,caption = paste("Results: Mathscore vs predictors (n = ",nobs(MM),")",sep = "")))
}

pred = paste(c("BOOKS","LIKES","GENDER","BELONG","TEASUPPORT",
                "AMBITIONS","PARENTSP","WORRY","TEABUL","DISCORD" ),collapse = "+")

#PISA_Mod1<- lmer(MATHSCORE~BOOKS+LIKES+GENDER+BELONG+TEASUPPORT+
                #AMBITIONS+PARENTSP+WORRY+TEABUL+DISCORD+(1|SCHOOLID),data =df_reg)

 
```


#### observe the coeficients

```{r}
my.dv= "MATHSCORE"
mlm.check(df=df_reg,my.dv)
```


```{r}
df=df_reg
my.model(df=df,dv=my.dv, pred = pred)
```

